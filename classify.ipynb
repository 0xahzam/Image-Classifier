{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using CLIP\n",
    "\n",
    "This is a self contained notebook using `openai/CLIP` to classify images from dataset of images inside a folder using a `token` and group them in another folder where name of folder having `token` as the folder name.\n",
    "\n",
    "For example, if `token = \"a bike\"` then all the images resembling the token, a bike, will be classified and grouped into a new folder in the parent directory named \"a bike\".\n",
    "\n",
    "### Folder Structure before running the script:\n",
    "- Parent Directory\n",
    "  - classify.ipynb\n",
    "  - images\n",
    "    - car.png\n",
    "    - bike.png\n",
    "    - bike2.png\n",
    "\n",
    "### Folder Structure after running the script:\n",
    "here `token = \"a bike\"`\n",
    "\n",
    "- Parent Directory\n",
    "  - classify.ipynb\n",
    "  - bike\n",
    "    - bike.png\n",
    "    - bike2.png\n",
    "  - images\n",
    "    - car.png"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [05:52<00:00, 1.01MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "\n",
    "def classify(img, token):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    image = preprocess(Image.open(img)).unsqueeze(0).to(device)\n",
    "    text = clip.tokenize([token, \" \"]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    return (probs[0][0])\n",
    "\n",
    "# assign directory\n",
    "directory = input(\"Enter the name of folder containing the images :\\n\")\n",
    "token=input(\"Enter token :\\n\")\n",
    "i = 0\n",
    "# iterate over files in\n",
    "# that directory\n",
    "map = dict()\n",
    "result = []\n",
    "\n",
    "for filename in os.scandir(directory):\n",
    "    if filename.is_file():\n",
    "        map[filename.path] = classify(filename.path, token)\n",
    "\n",
    "for i in map:\n",
    "    if map[i] > 0.8:\n",
    "        result.append(i)\n",
    "\n",
    "\n",
    "# creating path\n",
    "current_path = os.getcwd()\n",
    "path = current_path + \"\\\\\" + token\n",
    "os.mkdir(path)\n",
    "\n",
    "for i in result:\n",
    "    old_path = \"C:\\Files\\image classifier\\\\\" + i\n",
    "    new_path = path\n",
    "    shutil.move(old_path, new_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de2d1f70b8f213e2a3c67d3c5832fd85cf96929e55864415448fb74fa1ee732d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
